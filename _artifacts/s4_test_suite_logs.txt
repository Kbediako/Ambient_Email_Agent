--- Running Notebook Tests (pytest) ---
/usr/bin/python3: No module named pytest

--- Running Python Tests (run_all_tests.py) ---

Running tests for email_assistant...
   Project: E-mail Tool Calling and Response Evaluation

Running test_response.py for email_assistant...
   Experiment: Test: test_response.py | Agent: email_assistant

/usr/bin/python3: No module named pytest

--- Running Notebook Tests (pytest) ---
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 5 items

tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0] FAILED [ 20%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1] FAILED [ 40%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2] FAILED [ 60%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3] FAILED [ 80%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4] FAILED [100%]

=================================== FAILURES ===================================
______________ test_notebook_runs_without_errors[notebook_path0] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc414eba0>, {'cell_type': 'code', 'execution_coun...it_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 14)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8cc33853c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc414eba0>
cell = {'cell_type': 'code', 'execution_count': 7, 'id': '38308fc3', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 14
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': 'fcff5134-6ae8-492...e, 'engine': 'fcff5134-6ae8-4929-a96d-dd227b6075c1', 'started': '2025-08-23T15:44:25.918564Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel, Field
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.store.base import BaseStore
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl_memory, default_triage_instructions, default_background, default_response_preferences, default_cal_preferences, MEMORY_UPDATE_INSTRUCTIONS, MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT
E           from email_assistant.tools.default.prompt_templates import HITL_MEMORY_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E           
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E               
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/memory.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/memory.ipynb
______________ test_notebook_runs_without_errors[notebook_path1] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc32d0690>, {'cell_type': 'code', 'execution_coun...ges\'])\n            })\n\n    # Test passes if no expected calls are missing\n    assert len(missing_calls) == 0'}, 7)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8cc3387040>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc32d0690>
cell = {'cell_type': 'code', 'execution_count': 3, 'id': 'ae92fe30', 'metadata': {'execution': {'iopub.status.busy': '2025-08...essages\'])\n            })\n\n    # Test passes if no expected calls are missing\n    assert len(missing_calls) == 0'}
cell_index = 7
exec_reply = {'buffers': [], 'content': {'ename': 'DefaultCredentialsError', 'engine_info': {'engine_id': -1, 'engine_uuid': '87a0f...e, 'engine': '87a0fbd5-bb98-4514-b21e-9f4b4741697d', 'started': '2025-08-23T15:44:27.889558Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           import pytest
E           from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls
E           from email_assistant.utils import format_messages_string
E           from email_assistant.email_assistant import email_assistant
E           from email_assistant.utils import extract_tool_calls
E           
E           from langsmith import testing as t
E           
E           @pytest.mark.langsmith
E           @pytest.mark.parametrize(
E               "email_input, expected_calls",
E               [   # Pick some examples with e-mail reply expected
E                   (email_inputs[0],expected_tool_calls[0]),
E                   (email_inputs[3],expected_tool_calls[3]),
E               ],
E           )
E           def test_email_dataset_tool_calls(email_input, expected_calls):
E               """Test if email processing contains expected tool calls.
E               
E               This test confirms that all expected tools are called during email processing,
E               but does not check the order of tool invocations or the number of invocations
E               per tool. Additional checks for these aspects could be added if desired.
E               """
E               # Run the email assistant
E               messages = [{"role": "user", "content": str(email_input)}]
E               result = email_assistant.invoke({"messages": messages})
E                       
E               # Extract tool calls from messages list
E               extracted_tool_calls = extract_tool_calls(result['messages'])
E                       
E               # Check if all expected tool calls are in the extracted ones
E               missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]
E               
E               t.log_outputs({
E                           "missing_calls": missing_calls,
E                           "extracted_tool_calls": extracted_tool_calls,
E                           "response": format_messages_string(result['messages'])
E                       })
E           
E               # Test passes if no expected calls are missing
E               assert len(missing_calls) == 0
E           ------------------
E           
E           ----- stdout -----
E           [email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
E           ------------------
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[3][39m[32m, line 4[39m
E           [32m      2[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01meval[39;00m[34;01m.[39;00m[34;01memail_dataset[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_inputs, expected_tool_calls
E           [32m      3[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01mutils[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m format_messages_string
E           [32m----> [39m[32m4[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E           [32m      5[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01mutils[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m extract_tool_calls
E           [32m      7[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangsmith[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m testing [38;5;28;01mas[39;00m t
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant.py:40[39m
E           [32m     37[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33m[email_assistant] Models -> router=[39m[38;5;132;01m{[39;00mROUTER_MODEL_NAME[38;5;132;01m}[39;00m[33m, tools=[39m[38;5;132;01m{[39;00mTOOL_MODEL_NAME[38;5;132;01m}[39;00m[33m"[39m)
E           [32m     39[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m40[39m llm = [43mget_llm[49m[43m([49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m=[49m[43mROUTER_MODEL_NAME[49m[43m)[49m
E           [32m     41[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33m[email_assistant] Router model: [39m[38;5;132;01m{[39;00mROUTER_MODEL_NAME[38;5;132;01m}[39;00m[33m -> [39m[38;5;132;01m{[39;00m[38;5;28mtype[39m(llm).[34m__name__[39m[38;5;132;01m}[39;00m[33m"[39m)
E           [32m     42[39m llm_router = llm.with_structured_output(RouterSchema)
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E           [32m   1336[39m         suggestion = (
E           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E           [32m   1338[39m         )
E           [32m   1339[39m         logger.warning(
E           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E           [32m   1342[39m         )
E           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E           [32m   1408[39m [43m[49m[43m)[49m
E           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E           [32m    269[39m     config = _prepare_config(
E           [32m    270[39m         credentials=credentials,
E           [32m    271[39m         api_key=api_key,
E           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E           [32m    275[39m     )
E           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E           [32m    688[39m     transport_init: Union[
E           [32m    689[39m         Type[GenerativeServiceTransport],
E           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E           [32m    695[39m     )
E           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E           [32m    707[39m [43m    [49m[43m)[49m
E           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E           [32m    711[39m         std_logging.DEBUG
E           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E           [32m    230[39m                 certificate_chain=cert, private_key=key
E           [32m    231[39m             )
E           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E           [32m    243[39m [43m[49m[43m)[49m
E           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E           [32m     98[39m     )
E           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E           [32m    102[39m [43m    [49m[43m)[49m
E           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E           [32m    677[39m             _LOGGER.warning(
E           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E           [32m    681[39m                 environment_vars.PROJECT,
E           [32m    682[39m             )
E           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E           
E           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/evaluation.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[3][39m[32m, line 4[39m
E                           [32m      2[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01meval[39;00m[34;01m.[39;00m[34;01memail_dataset[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_inputs, expected_tool_calls
E                           [32m      3[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01mutils[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m format_messages_string
E                           [32m----> [39m[32m4[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E                           [32m      5[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01mutils[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m extract_tool_calls
E                           [32m      7[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangsmith[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m testing [38;5;28;01mas[39;00m t
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant.py:40[39m
E                           [32m     37[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33m[email_assistant] Models -> router=[39m[38;5;132;01m{[39;00mROUTER_MODEL_NAME[38;5;132;01m}[39;00m[33m, tools=[39m[38;5;132;01m{[39;00mTOOL_MODEL_NAME[38;5;132;01m}[39;00m[33m"[39m)
E                           [32m     39[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m40[39m llm = [43mget_llm[49m[43m([49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m=[49m[43mROUTER_MODEL_NAME[49m[43m)[49m
E                           [32m     41[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33m[email_assistant] Router model: [39m[38;5;132;01m{[39;00mROUTER_MODEL_NAME[38;5;132;01m}[39;00m[33m -> [39m[38;5;132;01m{[39;00m[38;5;28mtype[39m(llm).[34m__name__[39m[38;5;132;01m}[39;00m[33m"[39m)
E                           [32m     42[39m llm_router = llm.with_structured_output(RouterSchema)
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E                           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E                           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E                           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E                           [32m   1336[39m         suggestion = (
E                           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E                           [32m   1338[39m         )
E                           [32m   1339[39m         logger.warning(
E                           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E                           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E                           [32m   1342[39m         )
E                           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E                           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E                           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E                           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E                           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E                           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E                           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E                           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E                           [32m   1408[39m [43m[49m[43m)[49m
E                           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E                           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E                           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E                           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E                           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E                           [32m    269[39m     config = _prepare_config(
E                           [32m    270[39m         credentials=credentials,
E                           [32m    271[39m         api_key=api_key,
E                           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E                           [32m    275[39m     )
E                           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E                           [32m    688[39m     transport_init: Union[
E                           [32m    689[39m         Type[GenerativeServiceTransport],
E                           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E                           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E                           [32m    695[39m     )
E                           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E                           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E                           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E                           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E                           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E                           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E                           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E                           [32m    707[39m [43m    [49m[43m)[49m
E                           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E                           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E                           [32m    711[39m         std_logging.DEBUG
E                           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E                           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E                           [32m    230[39m                 certificate_chain=cert, private_key=key
E                           [32m    231[39m             )
E                           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E                           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E                           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E                           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E                           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E                           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E                           [32m    243[39m [43m[49m[43m)[49m
E                           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E                           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E                           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E                           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E                           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E                           [32m     98[39m     )
E                           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E                           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E                           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E                           [32m    102[39m [43m    [49m[43m)[49m
E                           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E                           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E                           [32m    677[39m             _LOGGER.warning(
E                           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E                           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E                           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E                           [32m    681[39m                 environment_vars.PROJECT,
E                           [32m    682[39m             )
E                           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E                           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E                           
E                           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/evaluation.ipynb
______________ test_notebook_runs_without_errors[notebook_path2] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc41cd310>, {'cell_type': 'code', 'execution_coun...nit_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 5)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8cc3387340>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc41cd310>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '6d4dfb07', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 5
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '392594f1-0b7d-4e8...e, 'engine': '392594f1-0b7d-4e85-bf4e-68418ed26b1a', 'started': '2025-08-23T15:44:29.801239Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences
E           from email_assistant.tools.default.prompt_templates import HITL_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           # This is new! 
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E               
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E           
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done,
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/hitl.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/hitl.ipynb
______________ test_notebook_runs_without_errors[notebook_path3] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc33c7950>, {'cell_type': 'code', 'execution_coun...join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}, 4)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8cc3387640>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc33c7950>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '072a8c76', 'metadata': {'execution': {'iopub.status.busy': '2025-08...ath.join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}
cell_index = 4
exec_reply = {'buffers': [], 'content': {'ename': 'DefaultCredentialsError', 'engine_info': {'engine_id': -1, 'engine_uuid': 'bddd7...e, 'engine': 'bddd7466-a4d3-4fd0-92dc-611127d4cfff', 'started': '2025-08-23T15:44:31.651004Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           import sys
E           import uuid
E           # Add src to path to allow for imports
E           sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))
E           
E           from email_assistant.email_assistant_hitl_memory_gmail import email_assistant
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E           [32m   1336[39m         suggestion = (
E           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E           [32m   1338[39m         )
E           [32m   1339[39m         logger.warning(
E           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E           [32m   1342[39m         )
E           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E           [32m   1408[39m [43m[49m[43m)[49m
E           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E           [32m    269[39m     config = _prepare_config(
E           [32m    270[39m         credentials=credentials,
E           [32m    271[39m         api_key=api_key,
E           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E           [32m    275[39m     )
E           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E           [32m    688[39m     transport_init: Union[
E           [32m    689[39m         Type[GenerativeServiceTransport],
E           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E           [32m    695[39m     )
E           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E           [32m    707[39m [43m    [49m[43m)[49m
E           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E           [32m    711[39m         std_logging.DEBUG
E           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E           [32m    230[39m                 certificate_chain=cert, private_key=key
E           [32m    231[39m             )
E           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E           [32m    243[39m [43m[49m[43m)[49m
E           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E           [32m     98[39m     )
E           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E           [32m    102[39m [43m    [49m[43m)[49m
E           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E           [32m    677[39m             _LOGGER.warning(
E           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E           [32m    681[39m                 environment_vars.PROJECT,
E           [32m    682[39m             )
E           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E           
E           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/agent.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E                           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E                           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E                           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E                           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E                           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E                           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E                           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E                           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E                           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E                           [32m   1336[39m         suggestion = (
E                           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E                           [32m   1338[39m         )
E                           [32m   1339[39m         logger.warning(
E                           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E                           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E                           [32m   1342[39m         )
E                           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E                           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E                           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E                           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E                           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E                           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E                           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E                           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E                           [32m   1408[39m [43m[49m[43m)[49m
E                           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E                           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E                           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E                           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E                           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E                           [32m    269[39m     config = _prepare_config(
E                           [32m    270[39m         credentials=credentials,
E                           [32m    271[39m         api_key=api_key,
E                           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E                           [32m    275[39m     )
E                           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E                           [32m    688[39m     transport_init: Union[
E                           [32m    689[39m         Type[GenerativeServiceTransport],
E                           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E                           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E                           [32m    695[39m     )
E                           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E                           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E                           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E                           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E                           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E                           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E                           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E                           [32m    707[39m [43m    [49m[43m)[49m
E                           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E                           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E                           [32m    711[39m         std_logging.DEBUG
E                           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E                           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E                           [32m    230[39m                 certificate_chain=cert, private_key=key
E                           [32m    231[39m             )
E                           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E                           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E                           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E                           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E                           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E                           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E                           [32m    243[39m [43m[49m[43m)[49m
E                           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E                           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E                           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E                           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E                           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E                           [32m     98[39m     )
E                           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E                           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E                           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E                           [32m    102[39m [43m    [49m[43m)[49m
E                           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E                           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E                           [32m    677[39m             _LOGGER.warning(
E                           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E                           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E                           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E                           [32m    681[39m                 environment_vars.PROJECT,
E                           [32m    682[39m             )
E                           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E                           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E                           
E                           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/agent.ipynb
______________ test_notebook_runs_without_errors[notebook_path4] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc412b360>, {'cell_type': 'code', 'execution_coun...urce': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}, 2)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8cc33853c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8cc412b360>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': 'e0ee8f6c', 'metadata': {'execution': {'iopub.status.busy': '2025-08... 'source': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}
cell_index = 2
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': 'b419a2d9-fc0a-4c9...e, 'engine': 'b419a2d9-fc0a-4c91-bc1e-73f115292d76', 'started': '2025-08-23T15:44:33.609037Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from langchain.chat_models import init_chat_model
E           llm = init_chat_model("openai:gpt-4.1", temperature=0)
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/langgraph_101.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E                           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E                           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/langgraph_101.ipynb
=============================== warnings summary ===============================
.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22
  /workspace/.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4]
======================== 5 failed, 1 warning in 13.06s =========================

--- Running Python Tests (run_all_tests.py) ---

Running tests for email_assistant...
   Project: E-mail Tool Calling and Response Evaluation

Running test_response.py for email_assistant...
   Experiment: Test: test_response.py | Agent: email_assistant
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 16 items

test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] ERROR [  6%]
test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] ERROR [ 12%]
test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] ERROR [ 18%]
test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] ERROR [ 25%]
test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] ERROR [ 31%]
test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] ERROR [ 37%]
test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] ERROR [ 43%]
test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] ERROR [ 50%]
test_response.py::test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] ERROR [ 56%]
test_response.py::test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] ERROR [ 62%]
test_response.py::test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] ERROR [ 68%]
test_response.py::test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] ERROR [ 75%]
test_response.py::test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] ERROR [ 81%]
test_response.py::test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] ERROR [ 87%]
test_response.py::test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] ERROR [ 93%]
test_response.py::test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] ERROR [100%]

==================================== ERRORS ====================================
_ ERROR at setup of test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
_ ERROR at setup of test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] _

agent_module_name = 'email_assistant'

    @pytest.fixture(autouse=True, scope="function")
    def set_agent_module(agent_module_name):
        """Set the global AGENT_MODULE for each test function.
        Using scope="function" ensures we get a fresh import for each test."""
        global AGENT_MODULE, agent_module
        AGENT_MODULE = agent_module_name
        print(f"Using agent module: {AGENT_MODULE}")
    
        # Force reload the module to ensure we get the latest code
        if f"email_assistant.{AGENT_MODULE}" in sys.modules:
            importlib.reload(sys.modules[f"email_assistant.{AGENT_MODULE}"])
    
>       agent_module = importlib.import_module(f"email_assistant.{AGENT_MODULE}")
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.13/importlib/__init__.py:88: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
<frozen importlib._bootstrap>:1331: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:935: in _load_unlocked
    ???
<frozen importlib._bootstrap_external>:1026: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../src/email_assistant/email_assistant.py:40: in <module>
    llm = get_llm(temperature=0.0, model=ROUTER_MODEL_NAME)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../src/email_assistant/configuration.py:22: in get_llm
    return ChatGoogleGenerativeAI(model=model_name, temperature=temperature, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343: in __init__
    super().__init__(**kwargs)
../.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130: in __init__
    super().__init__(*args, **kwargs)
../.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402: in validate_environment
    self.client = genaix.build_generative_service(
../.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276: in build_generative_service
    return v1betaGenerativeServiceClient(**config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697: in __init__
    self._transport = transport_init(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234: in __init__
    super().__init__(
../.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100: in __init__
    credentials, _ = google.auth.default(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scopes = None, request = None, quota_project_id = None, default_scopes = ()

    def default(scopes=None, request=None, quota_project_id=None, default_scopes=None):
        """Gets the default credentials for the current environment.
    
        `Application Default Credentials`_ provides an easy way to obtain
        credentials to call Google APIs for server-to-server or local applications.
        This function acquires credentials from the environment in the following
        order:
    
        1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set
           to the path of a valid service account JSON private key file, then it is
           loaded and returned. The project ID returned is the project ID defined
           in the service account file if available (some older files do not
           contain project ID information).
    
           If the environment variable is set to the path of a valid external
           account JSON configuration file (workload identity federation), then the
           configuration file is used to determine and retrieve the external
           credentials from the current environment (AWS, Azure, etc).
           These will then be exchanged for Google access tokens via the Google STS
           endpoint.
           The project ID returned in this case is the one corresponding to the
           underlying workload identity pool resource if determinable.
    
           If the environment variable is set to the path of a valid GDCH service
           account JSON file (`Google Distributed Cloud Hosted`_), then a GDCH
           credential will be returned. The project ID returned is the project
           specified in the JSON file.
        2. If the `Google Cloud SDK`_ is installed and has application default
           credentials set they are loaded and returned.
    
           To enable application default credentials with the Cloud SDK run::
    
                gcloud auth application-default login
    
           If the Cloud SDK has an active project, the project ID is returned. The
           active project can be set using::
    
                gcloud config set project
    
        3. If the application is running in the `App Engine standard environment`_
           (first generation) then the credentials and project ID from the
           `App Identity Service`_ are used.
        4. If the application is running in `Compute Engine`_ or `Cloud Run`_ or
           the `App Engine flexible environment`_ or the `App Engine standard
           environment`_ (second generation) then the credentials and project ID
           are obtained from the `Metadata Service`_.
        5. If no credentials are found,
           :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.
    
        .. _Application Default Credentials: https://developers.google.com\
                /identity/protocols/application-default-credentials
        .. _Google Cloud SDK: https://cloud.google.com/sdk
        .. _App Engine standard environment: https://cloud.google.com/appengine
        .. _App Identity Service: https://cloud.google.com/appengine/docs/python\
                /appidentity/
        .. _Compute Engine: https://cloud.google.com/compute
        .. _App Engine flexible environment: https://cloud.google.com\
                /appengine/flexible
        .. _Metadata Service: https://cloud.google.com/compute/docs\
                /storing-retrieving-metadata
        .. _Cloud Run: https://cloud.google.com/run
        .. _Google Distributed Cloud Hosted: https://cloud.google.com/blog/topics\
                /hybrid-cloud/announcing-google-distributed-cloud-edge-and-hosted
    
        Example::
    
            import google.auth
    
            credentials, project_id = google.auth.default()
    
        Args:
            scopes (Sequence[str]): The list of scopes for the credentials. If
                specified, the credentials will automatically be scoped if
                necessary.
            request (Optional[google.auth.transport.Request]): An object used to make
                HTTP requests. This is used to either detect whether the application
                is running on Compute Engine or to determine the associated project
                ID for a workload identity pool resource (external account
                credentials). If not specified, then it will either use the standard
                library http client to make requests for Compute Engine credentials
                or a google.auth.transport.requests.Request client for external
                account credentials.
            quota_project_id (Optional[str]): The project ID used for
                quota and billing.
            default_scopes (Optional[Sequence[str]]): Default scopes passed by a
                Google client library. Use 'scopes' for user-defined scopes.
        Returns:
            Tuple[~google.auth.credentials.Credentials, Optional[str]]:
                the current environment's credentials and project ID. Project ID
                may be None, which indicates that the Project ID could not be
                ascertained from the environment.
    
        Raises:
            ~google.auth.exceptions.DefaultCredentialsError:
                If no credentials were found, or if the credentials found were
                invalid.
        """
        from google.auth.credentials import with_scopes_if_required
        from google.auth.credentials import CredentialsWithQuotaProject
    
        explicit_project_id = os.environ.get(
            environment_vars.PROJECT, os.environ.get(environment_vars.LEGACY_PROJECT)
        )
    
        checkers = (
            # Avoid passing scopes here to prevent passing scopes to user credentials.
            # with_scopes_if_required() below will ensure scopes/default scopes are
            # safely set on the returned credentials since requires_scopes will
            # guard against setting scopes on user credentials.
            lambda: _get_explicit_environ_credentials(quota_project_id=quota_project_id),
            lambda: _get_gcloud_sdk_credentials(quota_project_id=quota_project_id),
            _get_gae_credentials,
            lambda: _get_gce_credentials(request, quota_project_id=quota_project_id),
        )
    
        for checker in checkers:
            credentials, project_id = checker()
            if credentials is not None:
                credentials = with_scopes_if_required(
                    credentials, scopes, default_scopes=default_scopes
                )
    
                effective_project_id = explicit_project_id or project_id
    
                # For external account credentials, scopes are required to determine
                # the project ID. Try to get the project ID again if not yet
                # determined.
                if not effective_project_id and callable(
                    getattr(credentials, "get_project_id", None)
                ):
                    if request is None:
                        import google.auth.transport.requests
    
                        request = google.auth.transport.requests.Request()
                    effective_project_id = credentials.get_project_id(request=request)
    
                if quota_project_id and isinstance(
                    credentials, CredentialsWithQuotaProject
                ):
                    credentials = credentials.with_quota_project(quota_project_id)
    
                if not effective_project_id:
                    _LOGGER.warning(
                        "No project ID could be determined. Consider running "
                        "`gcloud config set project` or setting the %s "
                        "environment variable",
                        environment_vars.PROJECT,
                    )
                return credentials, effective_project_id
    
>       raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E       google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

../.venv/lib/python3.13/site-packages/google/auth/_default.py:685: DefaultCredentialsError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
=========================== short test summary info ============================
ERROR test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0]
ERROR test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1]
ERROR test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2]
ERROR test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3]
ERROR test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4]
ERROR test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5]
ERROR test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6]
ERROR test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7]
ERROR test_response.py::test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0]
ERROR test_response.py::test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1]
ERROR test_response.py::test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2]
ERROR test_response.py::test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3]
ERROR test_response.py::test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4]
ERROR test_response.py::test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5]
ERROR test_response.py::test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6]
ERROR test_response.py::test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7]
======================== 2 warnings, 16 errors in 1.44s ========================

--- Running Notebook Tests (pytest) ---
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 5 items

tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0] FAILED [ 20%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1] FAILED [ 40%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2] FAILED [ 60%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3] FAILED [ 80%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4] FAILED [100%]

=================================== FAILURES ===================================
______________ test_notebook_runs_without_errors[notebook_path0] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5601826ba0>, {'cell_type': 'code', 'execution_coun...it_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 14)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f5600a59fc0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5601826ba0>
cell = {'cell_type': 'code', 'execution_count': 7, 'id': '38308fc3', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 14
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '9ac0fb5e-30b5-453...e, 'engine': '9ac0fb5e-30b5-4537-9dc4-dde82a7f74de', 'started': '2025-08-23T15:46:40.557218Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel, Field
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.store.base import BaseStore
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl_memory, default_triage_instructions, default_background, default_response_preferences, default_cal_preferences, MEMORY_UPDATE_INSTRUCTIONS, MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT
E           from email_assistant.tools.default.prompt_templates import HITL_MEMORY_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E           
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E               
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/memory.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/memory.ipynb
______________ test_notebook_runs_without_errors[notebook_path1] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5600944690>, {'cell_type': 'code', 'execution_coun...\n    # Add examples to the dataset\n    client.create_examples(dataset_id=dataset.id, examples=examples_triage)'}, 11)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f5600a5b640>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5600944690>
cell = {'cell_type': 'code', 'execution_count': 4, 'id': '7ea997ac', 'metadata': {'execution': {'iopub.status.busy': '2025-08...    )\n    # Add examples to the dataset\n    client.create_examples(dataset_id=dataset.id, examples=examples_triage)'}
cell_index = 11
exec_reply = {'buffers': [], 'content': {'ename': 'LangSmithAuthError', 'engine_info': {'engine_id': -1, 'engine_uuid': '6bdd559a-c...e, 'engine': '6bdd559a-c4a9-4897-a766-c0fd19ba1e43', 'started': '2025-08-23T15:46:43.195414Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from langsmith import Client
E           
E           from email_assistant.eval.email_dataset import examples_triage
E           
E           # Initialize LangSmith client
E           client = Client()
E           
E           # Dataset name
E           dataset_name = "E-mail Triage Evaluation"
E           
E           # Create dataset if it doesn't exist
E           if not client.has_dataset(dataset_name=dataset_name):
E               dataset = client.create_dataset(
E                   dataset_name=dataset_name, 
E                   description="A dataset of e-mails and their triage decisions."
E               )
E               # Add examples to the dataset
E               client.create_examples(dataset_id=dataset.id, examples=examples_triage)
E           ------------------
E           
E           ----- stderr -----
E           /workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API
E             warnings.warn(
E           ------------------
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mHTTPError[39m                                 Traceback (most recent call last)
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:154[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E           [32m    153[39m [38;5;28;01mtry[39;00m:
E           [32m--> [39m[32m154[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
E           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
E           [32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
E           [32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)
E           
E           [31mHTTPError[39m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation
E           
E           The above exception was the direct cause of the following exception:
E           
E           [31mHTTPError[39m                                 Traceback (most recent call last)
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:875[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E           [32m    869[39m     response = [38;5;28mself[39m.session.request(
E           [32m    870[39m         method,
E           [32m    871[39m         _construct_url([38;5;28mself[39m.api_url, pathname),
E           [32m    872[39m         stream=[38;5;28;01mFalse[39;00m,
E           [32m    873[39m         **request_kwargs,
E           [32m    874[39m     )
E           [32m--> [39m[32m875[39m [43mls_utils[49m[43m.[49m[43mraise_for_status_with_text[49m[43m([49m[43mresponse[49m[43m)[49m
E           [32m    876[39m [38;5;28;01mreturn[39;00m response
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:156[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E           [32m--> [39m[32m156[39m     [38;5;28;01mraise[39;00m requests.HTTPError([38;5;28mstr[39m(e), response.text) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m  [38;5;66;03m# type: ignore[call-arg][39;00m
E           [32m    157[39m [38;5;28;01mexcept[39;00m httpx.HTTPStatusError [38;5;28;01mas[39;00m e:
E           
E           [31mHTTPError[39m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation] {"detail":"Invalid token"}
E           
E           During handling of the above exception, another exception occurred:
E           
E           [31mLangSmithAuthError[39m                        Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[4][39m[32m, line 12[39m
E           [32m      9[39m dataset_name = [33m"[39m[33mE-mail Triage Evaluation[39m[33m"[39m
E           [32m     11[39m [38;5;66;03m# Create dataset if it doesn't exist[39;00m
E           [32m---> [39m[32m12[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [43mclient[49m[43m.[49m[43mhas_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m)[49m:
E           [32m     13[39m     dataset = client.create_dataset(
E           [32m     14[39m         dataset_name=dataset_name, 
E           [32m     15[39m         description=[33m"[39m[33mA dataset of e-mails and their triage decisions.[39m[33m"[39m
E           [32m     16[39m     )
E           [32m     17[39m     [38;5;66;03m# Add examples to the dataset[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3743[39m, in [36mClient.has_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E           [32m   3731[39m [38;5;250m[39m[33;03m"""Check whether a dataset exists in your tenant.[39;00m
E           [32m   3732[39m 
E           [32m   3733[39m [33;03mArgs:[39;00m
E           [32m   (...)[39m[32m   3740[39m [33;03m    bool: Whether the dataset exists.[39;00m
E           [32m   3741[39m [33;03m"""[39;00m
E           [32m   3742[39m [38;5;28;01mtry[39;00m:
E           [32m-> [39m[32m3743[39m     [38;5;28;43mself[39;49m[43m.[49m[43mread_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m,[49m[43m [49m[43mdataset_id[49m[43m=[49m[43mdataset_id[49m[43m)[49m
E           [32m   3744[39m     [38;5;28;01mreturn[39;00m [38;5;28;01mTrue[39;00m
E           [32m   3745[39m [38;5;28;01mexcept[39;00m ls_utils.LangSmithNotFoundError:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:142[39m, in [36mxor_args.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
E           [32m    136[39m     invalid_group_names = [[33m"[39m[33m, [39m[33m"[39m.join(arg_groups[i]) [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m invalid_groups]
E           [32m    137[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
E           [32m    138[39m         [33m"[39m[33mExactly one argument in each of the following[39m[33m"[39m
E           [32m    139[39m         [33m"[39m[33m groups must be defined:[39m[33m"[39m
E           [32m    140[39m         [33mf[39m[33m"[39m[33m [39m[38;5;132;01m{[39;00m[33m'[39m[33m, [39m[33m'[39m.join(invalid_group_names)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    141[39m     )
E           [32m--> [39m[32m142[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3774[39m, in [36mClient.read_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E           [32m   3772[39m [38;5;28;01melse[39;00m:
E           [32m   3773[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mMust provide dataset_name or dataset_id[39m[33m"[39m)
E           [32m-> [39m[32m3774[39m response = [38;5;28;43mself[39;49m[43m.[49m[43mrequest_with_retries[49m[43m([49m
E           [32m   3775[39m [43m    [49m[33;43m"[39;49m[33;43mGET[39;49m[33;43m"[39;49m[43m,[49m
E           [32m   3776[39m [43m    [49m[43mpath[49m[43m,[49m
E           [32m   3777[39m [43m    [49m[43mparams[49m[43m=[49m[43mparams[49m[43m,[49m
E           [32m   3778[39m [43m[49m[43m)[49m
E           [32m   3779[39m result = response.json()
E           [32m   3780[39m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, [38;5;28mlist[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:910[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E           [32m    905[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithRateLimitError(
E           [32m    906[39m         [33mf[39m[33m"[39m[33mRate limit exceeded for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    907[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    908[39m     )
E           [32m    909[39m [38;5;28;01melif[39;00m response.status_code == [32m401[39m:
E           [32m--> [39m[32m910[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithAuthError(
E           [32m    911[39m         [33mf[39m[33m"[39m[33mAuthentication failed for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    912[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    913[39m     )
E           [32m    914[39m [38;5;28;01melif[39;00m response.status_code == [32m404[39m:
E           [32m    915[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithNotFoundError(
E           [32m    916[39m         [33mf[39m[33m"[39m[33mResource not found for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    917[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    918[39m     )
E           
E           [31mLangSmithAuthError[39m: Authentication failed for /datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation', '{"detail":"Invalid token"}')

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/evaluation.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mHTTPError[39m                                 Traceback (most recent call last)
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:154[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E                           [32m    153[39m [38;5;28;01mtry[39;00m:
E                           [32m--> [39m[32m154[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
E                           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
E                           [32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
E                           [32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)
E                           
E                           [31mHTTPError[39m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation
E                           
E                           The above exception was the direct cause of the following exception:
E                           
E                           [31mHTTPError[39m                                 Traceback (most recent call last)
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:875[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E                           [32m    869[39m     response = [38;5;28mself[39m.session.request(
E                           [32m    870[39m         method,
E                           [32m    871[39m         _construct_url([38;5;28mself[39m.api_url, pathname),
E                           [32m    872[39m         stream=[38;5;28;01mFalse[39;00m,
E                           [32m    873[39m         **request_kwargs,
E                           [32m    874[39m     )
E                           [32m--> [39m[32m875[39m [43mls_utils[49m[43m.[49m[43mraise_for_status_with_text[49m[43m([49m[43mresponse[49m[43m)[49m
E                           [32m    876[39m [38;5;28;01mreturn[39;00m response
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:156[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E                           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E                           [32m--> [39m[32m156[39m     [38;5;28;01mraise[39;00m requests.HTTPError([38;5;28mstr[39m(e), response.text) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m  [38;5;66;03m# type: ignore[call-arg][39;00m
E                           [32m    157[39m [38;5;28;01mexcept[39;00m httpx.HTTPStatusError [38;5;28;01mas[39;00m e:
E                           
E                           [31mHTTPError[39m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation] {"detail":"Invalid token"}
E                           
E                           During handling of the above exception, another exception occurred:
E                           
E                           [31mLangSmithAuthError[39m                        Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[4][39m[32m, line 12[39m
E                           [32m      9[39m dataset_name = [33m"[39m[33mE-mail Triage Evaluation[39m[33m"[39m
E                           [32m     11[39m [38;5;66;03m# Create dataset if it doesn't exist[39;00m
E                           [32m---> [39m[32m12[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [43mclient[49m[43m.[49m[43mhas_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m)[49m:
E                           [32m     13[39m     dataset = client.create_dataset(
E                           [32m     14[39m         dataset_name=dataset_name, 
E                           [32m     15[39m         description=[33m"[39m[33mA dataset of e-mails and their triage decisions.[39m[33m"[39m
E                           [32m     16[39m     )
E                           [32m     17[39m     [38;5;66;03m# Add examples to the dataset[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3743[39m, in [36mClient.has_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E                           [32m   3731[39m [38;5;250m[39m[33;03m"""Check whether a dataset exists in your tenant.[39;00m
E                           [32m   3732[39m 
E                           [32m   3733[39m [33;03mArgs:[39;00m
E                           [32m   (...)[39m[32m   3740[39m [33;03m    bool: Whether the dataset exists.[39;00m
E                           [32m   3741[39m [33;03m"""[39;00m
E                           [32m   3742[39m [38;5;28;01mtry[39;00m:
E                           [32m-> [39m[32m3743[39m     [38;5;28;43mself[39;49m[43m.[49m[43mread_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m,[49m[43m [49m[43mdataset_id[49m[43m=[49m[43mdataset_id[49m[43m)[49m
E                           [32m   3744[39m     [38;5;28;01mreturn[39;00m [38;5;28;01mTrue[39;00m
E                           [32m   3745[39m [38;5;28;01mexcept[39;00m ls_utils.LangSmithNotFoundError:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:142[39m, in [36mxor_args.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
E                           [32m    136[39m     invalid_group_names = [[33m"[39m[33m, [39m[33m"[39m.join(arg_groups[i]) [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m invalid_groups]
E                           [32m    137[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
E                           [32m    138[39m         [33m"[39m[33mExactly one argument in each of the following[39m[33m"[39m
E                           [32m    139[39m         [33m"[39m[33m groups must be defined:[39m[33m"[39m
E                           [32m    140[39m         [33mf[39m[33m"[39m[33m [39m[38;5;132;01m{[39;00m[33m'[39m[33m, [39m[33m'[39m.join(invalid_group_names)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    141[39m     )
E                           [32m--> [39m[32m142[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3774[39m, in [36mClient.read_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E                           [32m   3772[39m [38;5;28;01melse[39;00m:
E                           [32m   3773[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mMust provide dataset_name or dataset_id[39m[33m"[39m)
E                           [32m-> [39m[32m3774[39m response = [38;5;28;43mself[39;49m[43m.[49m[43mrequest_with_retries[49m[43m([49m
E                           [32m   3775[39m [43m    [49m[33;43m"[39;49m[33;43mGET[39;49m[33;43m"[39;49m[43m,[49m
E                           [32m   3776[39m [43m    [49m[43mpath[49m[43m,[49m
E                           [32m   3777[39m [43m    [49m[43mparams[49m[43m=[49m[43mparams[49m[43m,[49m
E                           [32m   3778[39m [43m[49m[43m)[49m
E                           [32m   3779[39m result = response.json()
E                           [32m   3780[39m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, [38;5;28mlist[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:910[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E                           [32m    905[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithRateLimitError(
E                           [32m    906[39m         [33mf[39m[33m"[39m[33mRate limit exceeded for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    907[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    908[39m     )
E                           [32m    909[39m [38;5;28;01melif[39;00m response.status_code == [32m401[39m:
E                           [32m--> [39m[32m910[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithAuthError(
E                           [32m    911[39m         [33mf[39m[33m"[39m[33mAuthentication failed for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    912[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    913[39m     )
E                           [32m    914[39m [38;5;28;01melif[39;00m response.status_code == [32m404[39m:
E                           [32m    915[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithNotFoundError(
E                           [32m    916[39m         [33mf[39m[33m"[39m[33mResource not found for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    917[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    918[39m     )
E                           
E                           [31mLangSmithAuthError[39m: Authentication failed for /datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation', '{"detail":"Invalid token"}')

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/evaluation.ipynb
______________ test_notebook_runs_without_errors[notebook_path2] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f560189d590>, {'cell_type': 'code', 'execution_coun...nit_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 5)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f5600a5b4c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f560189d590>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '6d4dfb07', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 5
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '633b19a7-4efb-40e...e, 'engine': '633b19a7-4efb-40e9-b5cd-5fdb4756127d', 'started': '2025-08-23T15:46:44.942609Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences
E           from email_assistant.tools.default.prompt_templates import HITL_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           # This is new! 
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E               
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E           
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done,
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/hitl.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/hitl.ipynb
______________ test_notebook_runs_without_errors[notebook_path3] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5600a87360>, {'cell_type': 'code', 'execution_coun...join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}, 4)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f5600a5bc40>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f5600a87360>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '072a8c76', 'metadata': {'execution': {'iopub.status.busy': '2025-08...ath.join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}
cell_index = 4
exec_reply = {'buffers': [], 'content': {'ename': 'DefaultCredentialsError', 'engine_info': {'engine_id': -1, 'engine_uuid': '4b53f...e, 'engine': '4b53f7aa-614e-4b6c-9728-567fb4c9311d', 'started': '2025-08-23T15:46:46.945858Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           import sys
E           import uuid
E           # Add src to path to allow for imports
E           sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))
E           
E           from email_assistant.email_assistant_hitl_memory_gmail import email_assistant
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E           [32m   1336[39m         suggestion = (
E           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E           [32m   1338[39m         )
E           [32m   1339[39m         logger.warning(
E           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E           [32m   1342[39m         )
E           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E           [32m   1408[39m [43m[49m[43m)[49m
E           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E           [32m    269[39m     config = _prepare_config(
E           [32m    270[39m         credentials=credentials,
E           [32m    271[39m         api_key=api_key,
E           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E           [32m    275[39m     )
E           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E           [32m    688[39m     transport_init: Union[
E           [32m    689[39m         Type[GenerativeServiceTransport],
E           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E           [32m    695[39m     )
E           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E           [32m    707[39m [43m    [49m[43m)[49m
E           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E           [32m    711[39m         std_logging.DEBUG
E           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E           [32m    230[39m                 certificate_chain=cert, private_key=key
E           [32m    231[39m             )
E           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E           [32m    243[39m [43m[49m[43m)[49m
E           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E           [32m     98[39m     )
E           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E           [32m    102[39m [43m    [49m[43m)[49m
E           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E           [32m    677[39m             _LOGGER.warning(
E           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E           [32m    681[39m                 environment_vars.PROJECT,
E           [32m    682[39m             )
E           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E           
E           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/agent.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E                           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E                           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E                           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E                           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E                           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E                           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E                           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E                           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E                           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E                           [32m   1336[39m         suggestion = (
E                           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E                           [32m   1338[39m         )
E                           [32m   1339[39m         logger.warning(
E                           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E                           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E                           [32m   1342[39m         )
E                           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E                           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E                           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E                           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E                           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E                           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E                           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E                           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E                           [32m   1408[39m [43m[49m[43m)[49m
E                           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E                           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E                           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E                           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E                           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E                           [32m    269[39m     config = _prepare_config(
E                           [32m    270[39m         credentials=credentials,
E                           [32m    271[39m         api_key=api_key,
E                           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E                           [32m    275[39m     )
E                           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E                           [32m    688[39m     transport_init: Union[
E                           [32m    689[39m         Type[GenerativeServiceTransport],
E                           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E                           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E                           [32m    695[39m     )
E                           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E                           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E                           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E                           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E                           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E                           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E                           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E                           [32m    707[39m [43m    [49m[43m)[49m
E                           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E                           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E                           [32m    711[39m         std_logging.DEBUG
E                           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E                           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E                           [32m    230[39m                 certificate_chain=cert, private_key=key
E                           [32m    231[39m             )
E                           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E                           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E                           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E                           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E                           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E                           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E                           [32m    243[39m [43m[49m[43m)[49m
E                           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E                           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E                           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E                           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E                           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E                           [32m     98[39m     )
E                           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E                           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E                           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E                           [32m    102[39m [43m    [49m[43m)[49m
E                           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E                           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E                           [32m    677[39m             _LOGGER.warning(
E                           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E                           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E                           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E                           [32m    681[39m                 environment_vars.PROJECT,
E                           [32m    682[39m             )
E                           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E                           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E                           
E                           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/agent.ipynb
______________ test_notebook_runs_without_errors[notebook_path4] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f56028ecb00>, {'cell_type': 'code', 'execution_coun...urce': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}, 2)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f5600a5b640>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f56028ecb00>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': 'e0ee8f6c', 'metadata': {'execution': {'iopub.status.busy': '2025-08... 'source': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}
cell_index = 2
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '83119acc-5d49-46d...e, 'engine': '83119acc-5d49-46d1-a6d3-1c82be58ca91', 'started': '2025-08-23T15:46:48.887185Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from langchain.chat_models import init_chat_model
E           llm = init_chat_model("openai:gpt-4.1", temperature=0)
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/langgraph_101.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E                           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E                           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/langgraph_101.ipynb
=============================== warnings summary ===============================
.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22
  /workspace/.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4]
======================== 5 failed, 1 warning in 13.80s =========================

--- Running Python Tests (run_all_tests.py) ---

Running tests for email_assistant...
   Project: E-mail Tool Calling and Response Evaluation

Running test_response.py for email_assistant...
   Experiment: Test: test_response.py | Agent: email_assistant
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 16 items

test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] FAILED [  6%]
test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] FAILED [ 12%]
test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] FAILED [ 18%]
test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] FAILED [ 25%]
test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] FAILED [ 31%]
test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] FAILED [ 37%]
test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] FAILED [ 43%]
test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] FAILED [ 50%]
test_response.py::test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] SKIPPED [ 56%]
test_response.py::test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] SKIPPED [ 62%]
test_response.py::test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] SKIPPED [ 68%]
test_response.py::test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] SKIPPED [ 75%]
test_response.py::test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] SKIPPED [ 81%]
test_response.py::test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] SKIPPED [ 87%]
test_response.py::test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] SKIPPED [ 93%]
test_response.py::test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] SKIPPED [100%]

=================================== FAILURES ===================================
_ test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] _

email_input = {'author': 'Alice Smith <alice.smith@company.com>', 'email_thread': "Hi Lance,\n\nI was reviewing the API documentatio...date\n\nThanks!\nAlice", 'subject': 'Quick question about API documentation', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_1'
criteria = '\n• Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n'
expected_calls = ['write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f64a3d7f0>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Alice Smith <alice.smith@company.com>', 'email_threa...e\n\nThanks!\nAlice\n\n---\n", additional_kwargs={}, response_metadata={}, id='001683f6-b4a2-49da-ab3e-bd5c485f6809')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f64a1b460>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id '0ecc6b24-d3db-df9d-324a-78c088f642e0'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_1...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] _

email_input = {'author': 'Project Manager <pm@client.com>', 'email_thread': "Lance,\n\nIt's tax season again, and I wanted to schedu....\n\nRegards,\nProject Manager", 'subject': "Tax season let's schedule call", 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_4'
criteria = '\n• Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool cal...ail with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n'
expected_calls = ['check_calendar_availability', 'schedule_meeting', 'write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f6490d480>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Project Manager <pm@client.com>', 'email_thread': "L...s,\nProject Manager\n\n---\n", additional_kwargs={}, response_metadata={}, id='adcb09b7-7224-4985-bfa5-09875a65584a')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f5f7bfd50>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id '5ae47745-d8f0-b64e-34e4-8c819c30a017'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_4...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] _

email_input = {'author': 'Conference Organizer <events@techconf.com>', 'email_thread': "Hi Lance,\n\nWe're reaching out to invite yo...nConference Organizers", 'subject': 'Do you want to attend this conference?', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_6'
criteria = '\n• Express interest in attending TechConf 2025\n• Ask specific questions about AI/ML workshops\n• Inquire about grou...t in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n'
expected_calls = ['write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f5f1f4c00>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Conference Organizer <events@techconf.com>', 'email_...nference Organizers\n\n---\n", additional_kwargs={}, response_metadata={}, id='f997a4b8-3846-455e-959b-5f51695921c0')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f5f10fc50>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id '6e1fd754-3cd2-25e5-c4a1-684309426327'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_6...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] _

email_input = {'author': 'Sarah Johnson <sarah.j@partner.com>', 'email_thread': "Lance,\n\nI've attached the final version of our pr... advance,\nSarah", 'subject': 'Can you review these docs before submission?', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_7'
criteria = '\n• Explicitly agree to review the technical specifications\n• Acknowledge Friday deadline\n• Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n'
expected_calls = ['write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f5f1f45a0>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Sarah Johnson <sarah.j@partner.com>', 'email_thread'... in advance,\nSarah\n\n---\n", additional_kwargs={}, response_metadata={}, id='35732220-9ff0-4c4b-b658-dec45fe3cd35')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f5f6144d0>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id '0482c72f-7f53-fdc5-118d-66cd4ca68b4a'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_7...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] _

email_input = {'author': 'Community Pool <info@cityrecreation.org>', 'email_thread': "Dear Lance,\n\nSummer swimming registration is...ity Recreation Department", 'subject': 'Sign up daughter for swimming class', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_8'
criteria = '\n• Send email with write_email tool call to express interest in registering daughter for swimming class\n'
expected_calls = ['write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f64a1af10>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Community Pool <info@cityrecreation.org>', 'email_th...creation Department\n\n---\n", additional_kwargs={}, response_metadata={}, id='4a682395-4d99-4c39-8fd4-9dd9b1531f69')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f6492edd0>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id 'ccd69213-ba7f-7431-775e-832cdb8e5161'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_8...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] _

email_input = {'author': 'Team Lead <teamlead@company.com>', 'email_thread': "Hi Lance,\n\nIt's time for our quarterly planning sess...re priorities.\n\nBest,\nTeam Lead", 'subject': 'Quarterly planning meeting', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_10'
criteria = '\n• Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n• Send email acknowledging the request and providing availability with write_email tool call  \n'
expected_calls = ['check_calendar_availability', 'write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f64a1b790>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Team Lead <teamlead@company.com>', 'email_thread': "...n\nBest,\nTeam Lead\n\n---\n", additional_kwargs={}, response_metadata={}, id='2cd38ca9-c650-4be4-b034-900a0fedc63c')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f5f7bdc50>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id 'a9005169-6715-9474-d5ba-25521e3522d9'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_10...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] _

email_input = {'author': 'Dr. Roberts <droberts@medical.org>', 'email_thread': "Hello Lance,\n\nThis is a reminder that it's time fo...n\nBest regards,\nDr. Roberts' Office", 'subject': 'Annual checkup reminder', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_13'
criteria = '\n• Acknowledge annual checkup reminder\n• Send email with write_email tool call to acknowledge annual checkup reminder\n'
expected_calls = ['write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f64a186b0>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Dr. Roberts <droberts@medical.org>', 'email_thread':...Dr. Roberts' Office\n\n---\n", additional_kwargs={}, response_metadata={}, id='ee80e26f-7b3c-4a05-abd8-6df0385e3ad1')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f648f5950>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id 'a64afa7b-f853-de65-73ef-e9364c41dee9'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_13...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
_ test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] _

email_input = {'author': 'Project Team <project@company.com>', 'email_thread': "Hi Lance,\n\nThe leadership team has asked us to pre...sdays.\n\nThanks,\nProject Team", 'subject': 'Joint presentation next month', 'to': 'Lance Martin <lance@company.com>'}
email_name = 'email_input_15'
criteria = '\n• Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool c...o collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n'
expected_calls = ['check_calendar_availability', 'schedule_meeting', 'write_email', 'done']

    @pytest.mark.langsmith(output_keys=["expected_calls"])
    # Variable names and a list of tuples with the test cases
    @pytest.mark.parametrize("email_input,email_name,criteria,expected_calls",create_response_test_cases())
    def test_email_dataset_tool_calls(email_input, email_name, criteria, expected_calls):
        """Test if email processing contains expected tool calls."""
        # Log minimal inputs for LangSmith (safe noop if plugin disabled)
        try:
            t.log_inputs({"module": AGENT_MODULE, "test": "test_email_dataset_tool_calls"})
        except Exception:
            pass
    
        print(f"Processing {email_name}...")
    
        # Set up the assistant
        email_assistant, thread_config, _ = setup_assistant()
    
        # Run the agent
        if AGENT_MODULE in ["email_assistant", "email_assistant_hitl_memory_gmail"]:
            # Workflow agent takes email_input directly
>           result = email_assistant.invoke({"email_input": email_input}, config=thread_config)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

test_response.py:225: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2647: in stream
    for _ in runner.tick(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:162: in tick
    run_with_retry(
../.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:42: in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:657: in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026: in invoke
    for chunk in self.stream(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x7f4f5f1f4f30>
input = {'classification_decision': 'respond', 'email_input': {'author': 'Project Team <project@company.com>', 'email_thread':...anks,\nProject Team\n\n---\n", additional_kwargs={}, response_metadata={}, id='23d984cb-3c7e-4f83-8c70-a1c681ce77f1')]}
config = {'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x7f4f5f3567d0>, 'configurable': {'__pregel_...graph_node': 'response_agent', 'langgraph_path': ('__pregel_pull', 'response_agent'), ...}, 'recursion_limit': 25, ...}
context = None, stream_mode = ['updates', 'values'], print_mode = ()
output_keys = ['messages', 'email_input', 'classification_decision']
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0."
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by get_state().
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes. Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to "async". Options are:
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(Optional[str], config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT
E               During task with name 'response_agent' and id 'de296d02-2172-202f-fcfd-4e7154ceb3f7'

../.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2675: GraphRecursionError
---------------------------- Captured stdout setup -----------------------------
Using agent module: email_assistant
[email_assistant] Models -> router=gemini-2.5-pro, tools=gemini-2.5-pro
⚠️ Router model unavailable; falling back to heuristics. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
⚠️ Tool model unavailable; using deterministic tool plan fallback. Details: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.
----------------------------- Captured stdout call -----------------------------
Processing email_input_15...
⚠️ Triage returned no/invalid classification after retries; using heuristic.
📧 Classification: RESPOND - This email requires a response
=========================== short test summary info ============================
FAILED test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0]
FAILED test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1]
FAILED test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2]
FAILED test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3]
FAILED test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4]
FAILED test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5]
FAILED test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6]
FAILED test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7]
================== 8 failed, 8 skipped, 18 warnings in 1.51s ===================

--- Running Notebook Tests (pytest) ---
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 5 items

tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0] FAILED [ 20%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1] FAILED [ 40%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2] FAILED [ 60%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3] FAILED [ 80%]
tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4] FAILED [100%]

=================================== FAILURES ===================================
______________ test_notebook_runs_without_errors[notebook_path0] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf999aba0>, {'cell_type': 'code', 'execution_coun...it_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 14)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8bf8db1fc0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf999aba0>
cell = {'cell_type': 'code', 'execution_count': 7, 'id': '38308fc3', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 14
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '7a74b536-522a-4ea...e, 'engine': '7a74b536-522a-4ea3-9e2b-90f3520e32f7', 'started': '2025-08-23T15:51:07.644845Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel, Field
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.store.base import BaseStore
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl_memory, default_triage_instructions, default_background, default_response_preferences, default_cal_preferences, MEMORY_UPDATE_INSTRUCTIONS, MEMORY_UPDATE_INSTRUCTIONS_REINFORCEMENT
E           from email_assistant.tools.default.prompt_templates import HITL_MEMORY_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E           
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E               
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/memory.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/memory.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[7][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/memory.ipynb
______________ test_notebook_runs_without_errors[notebook_path1] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf8cac550>, {'cell_type': 'code', 'execution_coun...\n    # Add examples to the dataset\n    client.create_examples(dataset_id=dataset.id, examples=examples_triage)'}, 11)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8bf8db3640>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf8cac550>
cell = {'cell_type': 'code', 'execution_count': 4, 'id': '7ea997ac', 'metadata': {'execution': {'iopub.status.busy': '2025-08...    )\n    # Add examples to the dataset\n    client.create_examples(dataset_id=dataset.id, examples=examples_triage)'}
cell_index = 11
exec_reply = {'buffers': [], 'content': {'ename': 'LangSmithAuthError', 'engine_info': {'engine_id': -1, 'engine_uuid': '9ba86e44-e...e, 'engine': '9ba86e44-edb9-49a6-8631-de7f39a3e33a', 'started': '2025-08-23T15:51:10.177943Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from langsmith import Client
E           
E           from email_assistant.eval.email_dataset import examples_triage
E           
E           # Initialize LangSmith client
E           client = Client()
E           
E           # Dataset name
E           dataset_name = "E-mail Triage Evaluation"
E           
E           # Create dataset if it doesn't exist
E           if not client.has_dataset(dataset_name=dataset_name):
E               dataset = client.create_dataset(
E                   dataset_name=dataset_name, 
E                   description="A dataset of e-mails and their triage decisions."
E               )
E               # Add examples to the dataset
E               client.create_examples(dataset_id=dataset.id, examples=examples_triage)
E           ------------------
E           
E           ----- stderr -----
E           /workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API
E             warnings.warn(
E           ------------------
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mHTTPError[39m                                 Traceback (most recent call last)
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:154[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E           [32m    153[39m [38;5;28;01mtry[39;00m:
E           [32m--> [39m[32m154[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
E           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
E           [32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
E           [32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)
E           
E           [31mHTTPError[39m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation
E           
E           The above exception was the direct cause of the following exception:
E           
E           [31mHTTPError[39m                                 Traceback (most recent call last)
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:875[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E           [32m    869[39m     response = [38;5;28mself[39m.session.request(
E           [32m    870[39m         method,
E           [32m    871[39m         _construct_url([38;5;28mself[39m.api_url, pathname),
E           [32m    872[39m         stream=[38;5;28;01mFalse[39;00m,
E           [32m    873[39m         **request_kwargs,
E           [32m    874[39m     )
E           [32m--> [39m[32m875[39m [43mls_utils[49m[43m.[49m[43mraise_for_status_with_text[49m[43m([49m[43mresponse[49m[43m)[49m
E           [32m    876[39m [38;5;28;01mreturn[39;00m response
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:156[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E           [32m--> [39m[32m156[39m     [38;5;28;01mraise[39;00m requests.HTTPError([38;5;28mstr[39m(e), response.text) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m  [38;5;66;03m# type: ignore[call-arg][39;00m
E           [32m    157[39m [38;5;28;01mexcept[39;00m httpx.HTTPStatusError [38;5;28;01mas[39;00m e:
E           
E           [31mHTTPError[39m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation] {"detail":"Invalid token"}
E           
E           During handling of the above exception, another exception occurred:
E           
E           [31mLangSmithAuthError[39m                        Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[4][39m[32m, line 12[39m
E           [32m      9[39m dataset_name = [33m"[39m[33mE-mail Triage Evaluation[39m[33m"[39m
E           [32m     11[39m [38;5;66;03m# Create dataset if it doesn't exist[39;00m
E           [32m---> [39m[32m12[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [43mclient[49m[43m.[49m[43mhas_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m)[49m:
E           [32m     13[39m     dataset = client.create_dataset(
E           [32m     14[39m         dataset_name=dataset_name, 
E           [32m     15[39m         description=[33m"[39m[33mA dataset of e-mails and their triage decisions.[39m[33m"[39m
E           [32m     16[39m     )
E           [32m     17[39m     [38;5;66;03m# Add examples to the dataset[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3743[39m, in [36mClient.has_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E           [32m   3731[39m [38;5;250m[39m[33;03m"""Check whether a dataset exists in your tenant.[39;00m
E           [32m   3732[39m 
E           [32m   3733[39m [33;03mArgs:[39;00m
E           [32m   (...)[39m[32m   3740[39m [33;03m    bool: Whether the dataset exists.[39;00m
E           [32m   3741[39m [33;03m"""[39;00m
E           [32m   3742[39m [38;5;28;01mtry[39;00m:
E           [32m-> [39m[32m3743[39m     [38;5;28;43mself[39;49m[43m.[49m[43mread_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m,[49m[43m [49m[43mdataset_id[49m[43m=[49m[43mdataset_id[49m[43m)[49m
E           [32m   3744[39m     [38;5;28;01mreturn[39;00m [38;5;28;01mTrue[39;00m
E           [32m   3745[39m [38;5;28;01mexcept[39;00m ls_utils.LangSmithNotFoundError:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:142[39m, in [36mxor_args.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
E           [32m    136[39m     invalid_group_names = [[33m"[39m[33m, [39m[33m"[39m.join(arg_groups[i]) [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m invalid_groups]
E           [32m    137[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
E           [32m    138[39m         [33m"[39m[33mExactly one argument in each of the following[39m[33m"[39m
E           [32m    139[39m         [33m"[39m[33m groups must be defined:[39m[33m"[39m
E           [32m    140[39m         [33mf[39m[33m"[39m[33m [39m[38;5;132;01m{[39;00m[33m'[39m[33m, [39m[33m'[39m.join(invalid_group_names)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    141[39m     )
E           [32m--> [39m[32m142[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3774[39m, in [36mClient.read_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E           [32m   3772[39m [38;5;28;01melse[39;00m:
E           [32m   3773[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mMust provide dataset_name or dataset_id[39m[33m"[39m)
E           [32m-> [39m[32m3774[39m response = [38;5;28;43mself[39;49m[43m.[49m[43mrequest_with_retries[49m[43m([49m
E           [32m   3775[39m [43m    [49m[33;43m"[39;49m[33;43mGET[39;49m[33;43m"[39;49m[43m,[49m
E           [32m   3776[39m [43m    [49m[43mpath[49m[43m,[49m
E           [32m   3777[39m [43m    [49m[43mparams[49m[43m=[49m[43mparams[49m[43m,[49m
E           [32m   3778[39m [43m[49m[43m)[49m
E           [32m   3779[39m result = response.json()
E           [32m   3780[39m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, [38;5;28mlist[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:910[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E           [32m    905[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithRateLimitError(
E           [32m    906[39m         [33mf[39m[33m"[39m[33mRate limit exceeded for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    907[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    908[39m     )
E           [32m    909[39m [38;5;28;01melif[39;00m response.status_code == [32m401[39m:
E           [32m--> [39m[32m910[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithAuthError(
E           [32m    911[39m         [33mf[39m[33m"[39m[33mAuthentication failed for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    912[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    913[39m     )
E           [32m    914[39m [38;5;28;01melif[39;00m response.status_code == [32m404[39m:
E           [32m    915[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithNotFoundError(
E           [32m    916[39m         [33mf[39m[33m"[39m[33mResource not found for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m    917[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E           [32m    918[39m     )
E           
E           [31mLangSmithAuthError[39m: Authentication failed for /datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation', '{"detail":"Invalid token"}')

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/evaluation.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/evaluation.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mHTTPError[39m                                 Traceback (most recent call last)
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:154[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E                           [32m    153[39m [38;5;28;01mtry[39;00m:
E                           [32m--> [39m[32m154[39m     [43mresponse[49m[43m.[49m[43mraise_for_status[49m[43m([49m[43m)[49m
E                           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/requests/models.py:1026[39m, in [36mResponse.raise_for_status[39m[34m(self)[39m
E                           [32m   1025[39m [38;5;28;01mif[39;00m http_error_msg:
E                           [32m-> [39m[32m1026[39m     [38;5;28;01mraise[39;00m HTTPError(http_error_msg, response=[38;5;28mself[39m)
E                           
E                           [31mHTTPError[39m: 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation
E                           
E                           The above exception was the direct cause of the following exception:
E                           
E                           [31mHTTPError[39m                                 Traceback (most recent call last)
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:875[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E                           [32m    869[39m     response = [38;5;28mself[39m.session.request(
E                           [32m    870[39m         method,
E                           [32m    871[39m         _construct_url([38;5;28mself[39m.api_url, pathname),
E                           [32m    872[39m         stream=[38;5;28;01mFalse[39;00m,
E                           [32m    873[39m         **request_kwargs,
E                           [32m    874[39m     )
E                           [32m--> [39m[32m875[39m [43mls_utils[49m[43m.[49m[43mraise_for_status_with_text[49m[43m([49m[43mresponse[49m[43m)[49m
E                           [32m    876[39m [38;5;28;01mreturn[39;00m response
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:156[39m, in [36mraise_for_status_with_text[39m[34m(response)[39m
E                           [32m    155[39m [38;5;28;01mexcept[39;00m requests.HTTPError [38;5;28;01mas[39;00m e:
E                           [32m--> [39m[32m156[39m     [38;5;28;01mraise[39;00m requests.HTTPError([38;5;28mstr[39m(e), response.text) [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01me[39;00m  [38;5;66;03m# type: ignore[call-arg][39;00m
E                           [32m    157[39m [38;5;28;01mexcept[39;00m httpx.HTTPStatusError [38;5;28;01mas[39;00m e:
E                           
E                           [31mHTTPError[39m: [Errno 401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation] {"detail":"Invalid token"}
E                           
E                           During handling of the above exception, another exception occurred:
E                           
E                           [31mLangSmithAuthError[39m                        Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[4][39m[32m, line 12[39m
E                           [32m      9[39m dataset_name = [33m"[39m[33mE-mail Triage Evaluation[39m[33m"[39m
E                           [32m     11[39m [38;5;66;03m# Create dataset if it doesn't exist[39;00m
E                           [32m---> [39m[32m12[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [43mclient[49m[43m.[49m[43mhas_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m)[49m:
E                           [32m     13[39m     dataset = client.create_dataset(
E                           [32m     14[39m         dataset_name=dataset_name, 
E                           [32m     15[39m         description=[33m"[39m[33mA dataset of e-mails and their triage decisions.[39m[33m"[39m
E                           [32m     16[39m     )
E                           [32m     17[39m     [38;5;66;03m# Add examples to the dataset[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3743[39m, in [36mClient.has_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E                           [32m   3731[39m [38;5;250m[39m[33;03m"""Check whether a dataset exists in your tenant.[39;00m
E                           [32m   3732[39m 
E                           [32m   3733[39m [33;03mArgs:[39;00m
E                           [32m   (...)[39m[32m   3740[39m [33;03m    bool: Whether the dataset exists.[39;00m
E                           [32m   3741[39m [33;03m"""[39;00m
E                           [32m   3742[39m [38;5;28;01mtry[39;00m:
E                           [32m-> [39m[32m3743[39m     [38;5;28;43mself[39;49m[43m.[49m[43mread_dataset[49m[43m([49m[43mdataset_name[49m[43m=[49m[43mdataset_name[49m[43m,[49m[43m [49m[43mdataset_id[49m[43m=[49m[43mdataset_id[49m[43m)[49m
E                           [32m   3744[39m     [38;5;28;01mreturn[39;00m [38;5;28;01mTrue[39;00m
E                           [32m   3745[39m [38;5;28;01mexcept[39;00m ls_utils.LangSmithNotFoundError:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/utils.py:142[39m, in [36mxor_args.<locals>.decorator.<locals>.wrapper[39m[34m(*args, **kwargs)[39m
E                           [32m    136[39m     invalid_group_names = [[33m"[39m[33m, [39m[33m"[39m.join(arg_groups[i]) [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m invalid_groups]
E                           [32m    137[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
E                           [32m    138[39m         [33m"[39m[33mExactly one argument in each of the following[39m[33m"[39m
E                           [32m    139[39m         [33m"[39m[33m groups must be defined:[39m[33m"[39m
E                           [32m    140[39m         [33mf[39m[33m"[39m[33m [39m[38;5;132;01m{[39;00m[33m'[39m[33m, [39m[33m'[39m.join(invalid_group_names)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    141[39m     )
E                           [32m--> [39m[32m142[39m [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:3774[39m, in [36mClient.read_dataset[39m[34m(self, dataset_name, dataset_id)[39m
E                           [32m   3772[39m [38;5;28;01melse[39;00m:
E                           [32m   3773[39m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([33m"[39m[33mMust provide dataset_name or dataset_id[39m[33m"[39m)
E                           [32m-> [39m[32m3774[39m response = [38;5;28;43mself[39;49m[43m.[49m[43mrequest_with_retries[49m[43m([49m
E                           [32m   3775[39m [43m    [49m[33;43m"[39;49m[33;43mGET[39;49m[33;43m"[39;49m[43m,[49m
E                           [32m   3776[39m [43m    [49m[43mpath[49m[43m,[49m
E                           [32m   3777[39m [43m    [49m[43mparams[49m[43m=[49m[43mparams[49m[43m,[49m
E                           [32m   3778[39m [43m[49m[43m)[49m
E                           [32m   3779[39m result = response.json()
E                           [32m   3780[39m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(result, [38;5;28mlist[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langsmith/client.py:910[39m, in [36mClient.request_with_retries[39m[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)[39m
E                           [32m    905[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithRateLimitError(
E                           [32m    906[39m         [33mf[39m[33m"[39m[33mRate limit exceeded for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    907[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    908[39m     )
E                           [32m    909[39m [38;5;28;01melif[39;00m response.status_code == [32m401[39m:
E                           [32m--> [39m[32m910[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithAuthError(
E                           [32m    911[39m         [33mf[39m[33m"[39m[33mAuthentication failed for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    912[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    913[39m     )
E                           [32m    914[39m [38;5;28;01melif[39;00m response.status_code == [32m404[39m:
E                           [32m    915[39m     [38;5;28;01mraise[39;00m ls_utils.LangSmithNotFoundError(
E                           [32m    916[39m         [33mf[39m[33m"[39m[33mResource not found for [39m[38;5;132;01m{[39;00mpathname[38;5;132;01m}[39;00m[33m. [39m[38;5;132;01m{[39;00m[38;5;28mrepr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    917[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00m_context[38;5;132;01m}[39;00m[33m"[39m
E                           [32m    918[39m     )
E                           
E                           [31mLangSmithAuthError[39m: Authentication failed for /datasets. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/datasets?limit=1&name=E-mail+Triage+Evaluation', '{"detail":"Invalid token"}')

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/evaluation.ipynb
______________ test_notebook_runs_without_errors[notebook_path2] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf9a0d450>, {'cell_type': 'code', 'execution_coun...nit_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}, 5)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8bf8db34c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf9a0d450>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '6d4dfb07', 'metadata': {'execution': {'iopub.status.busy': '2025-08... = init_chat_model("openai:gpt-4.1", temperature=0.0)\nllm_with_tools = llm.bind_tools(tools, tool_choice="required")'}
cell_index = 5
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '1b149b7e-36b4-46b...e, 'engine': '1b149b7e-36b4-46bf-b300-ee35c6d65b91', 'started': '2025-08-23T15:51:11.699060Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           
E           %load_ext autoreload
E           %autoreload 2
E           
E           from typing import Literal
E           from datetime import datetime
E           from pydantic import BaseModel
E           
E           from langchain.chat_models import init_chat_model
E           from langchain_core.tools import tool
E           
E           from langgraph.graph import StateGraph, START, END
E           from langgraph.types import interrupt, Command
E           
E           from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences
E           from email_assistant.tools.default.prompt_templates import HITL_TOOLS_PROMPT
E           from email_assistant.schemas import State, RouterSchema, StateInput
E           from email_assistant.utils import parse_email, format_for_display, format_email_markdown
E           
E           # Agent tools 
E           @tool
E           def write_email(to: str, subject: str, content: str) -> str:
E               """Write and send an email."""
E               # Placeholder response - in real app would send email
E               return f"Email sent to {to} with subject '{subject}' and content: {content}"
E           
E           @tool
E           def schedule_meeting(
E               attendees: list[str], subject: str, duration_minutes: int, preferred_day: datetime, start_time: int
E           ) -> str:
E               """Schedule a calendar meeting."""
E               # Placeholder response - in real app would check calendar and schedule
E               date_str = preferred_day.strftime("%A, %B %d, %Y")
E               return f"Meeting '{subject}' scheduled on {date_str} at {start_time} for {duration_minutes} minutes with {len(attendees)} attendees"
E           
E           @tool
E           def check_calendar_availability(day: str) -> str:
E               """Check calendar availability for a given day."""
E               # Placeholder response - in real app would check actual calendar
E               return f"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM"
E           
E           @tool
E           # This is new! 
E           class Question(BaseModel):
E                 """Question to ask user."""
E                 content: str
E               
E           @tool
E           class Done(BaseModel):
E                 """E-mail has been sent."""
E                 done: bool
E           
E           # All tools available to the agent
E           tools = [
E               write_email, 
E               schedule_meeting, 
E               check_calendar_availability, 
E               Question, 
E               Done,
E           ]
E           
E           tools_by_name = {tool.name: tool for tool in tools}
E           
E           # Initialize the LLM for use with router / structured output
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_router = llm.with_structured_output(RouterSchema) 
E           
E           # Initialize the LLM, enforcing tool use (of any available tools) for agent
E           llm = init_chat_model("openai:gpt-4.1", temperature=0.0)
E           llm_with_tools = llm.bind_tools(tools, tool_choice="required")
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/hitl.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/hitl.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 64[39m
E                           [32m     61[39m tools_by_name = {tool.name: tool [38;5;28;01mfor[39;00m tool [38;5;129;01min[39;00m tools}
E                           [32m     63[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m64[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0.0[39;49m[43m)[49m
E                           [32m     65[39m llm_router = llm.with_structured_output(RouterSchema) 
E                           [32m     67[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/hitl.ipynb
______________ test_notebook_runs_without_errors[notebook_path3] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf8df3230>, {'cell_type': 'code', 'execution_coun...join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}, 4)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8bf8db37c0>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bf8df3230>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': '072a8c76', 'metadata': {'execution': {'iopub.status.busy': '2025-08...ath.join(os.getcwd(), '..', 'src')))\n\nfrom email_assistant.email_assistant_hitl_memory_gmail import email_assistant"}
cell_index = 4
exec_reply = {'buffers': [], 'content': {'ename': 'DefaultCredentialsError', 'engine_info': {'engine_id': -1, 'engine_uuid': '87dd2...e, 'engine': '87dd27f5-5ae0-4c01-9265-972adeff2da1', 'started': '2025-08-23T15:51:13.695336Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           import sys
E           import uuid
E           # Add src to path to allow for imports
E           sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))
E           
E           from email_assistant.email_assistant_hitl_memory_gmail import email_assistant
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E           
E           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E           [32m   1336[39m         suggestion = (
E           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E           [32m   1338[39m         )
E           [32m   1339[39m         logger.warning(
E           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E           [32m   1342[39m         )
E           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E           [32m   1408[39m [43m[49m[43m)[49m
E           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E           [32m    269[39m     config = _prepare_config(
E           [32m    270[39m         credentials=credentials,
E           [32m    271[39m         api_key=api_key,
E           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E           [32m    275[39m     )
E           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E           [32m    688[39m     transport_init: Union[
E           [32m    689[39m         Type[GenerativeServiceTransport],
E           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E           [32m    695[39m     )
E           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E           [32m    707[39m [43m    [49m[43m)[49m
E           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E           [32m    711[39m         std_logging.DEBUG
E           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E           [32m    230[39m                 certificate_chain=cert, private_key=key
E           [32m    231[39m             )
E           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E           [32m    243[39m [43m[49m[43m)[49m
E           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E           [32m     98[39m     )
E           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E           [32m    102[39m [43m    [49m[43m)[49m
E           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E           [32m    677[39m             _LOGGER.warning(
E           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E           [32m    681[39m                 environment_vars.PROJECT,
E           [32m    682[39m             )
E           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E           
E           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/agent.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/agent.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mDefaultCredentialsError[39m                   Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 6[39m
E                           [32m      3[39m [38;5;66;03m# Add src to path to allow for imports[39;00m
E                           [32m      4[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), [33m'[39m[33m..[39m[33m'[39m, [33m'[39m[33msrc[39m[33m'[39m)))
E                           [32m----> [39m[32m6[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01memail_assistant[39;00m[34;01m.[39;00m[34;01memail_assistant_hitl_memory_gmail[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m email_assistant
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/email_assistant_hitl_memory_gmail.py:63[39m
E                           [32m     59[39m         [38;5;28;01mreturn[39;00m [33mf[39m[33m"[39m[33mError executing [39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[33m: [39m[38;5;132;01m{[39;00m[38;5;28mstr[39m(e)[38;5;132;01m}[39;00m[33m"[39m
E                           [32m     62[39m [38;5;66;03m# Initialize the LLM for use with router / structured output[39;00m
E                           [32m---> [39m[32m63[39m llm = [43mget_llm[49m[43m([49m[43m)[49m
E                           [32m     64[39m llm_router = llm.with_structured_output(RouterSchema)
E                           [32m     66[39m [38;5;66;03m# Initialize the LLM, enforcing tool use (of any available tools) for agent[39;00m
E                           
E                           [36mFile [39m[32m/workspace/src/email_assistant/configuration.py:22[39m, in [36mget_llm[39m[34m(temperature, **kwargs)[39m
E                           [32m     20[39m [38;5;28;01mif[39;00m model_name.startswith([33m"[39m[33mmodels/[39m[33m"[39m):
E                           [32m     21[39m     model_name = model_name.split([33m"[39m[33m/[39m[33m"[39m, [32m1[39m)[[32m1[39m]
E                           [32m---> [39m[32m22[39m [38;5;28;01mreturn[39;00m [43mChatGoogleGenerativeAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel_name[49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[43mtemperature[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1343[39m, in [36mChatGoogleGenerativeAI.__init__[39m[34m(self, **kwargs)[39m
E                           [32m   1336[39m         suggestion = (
E                           [32m   1337[39m             [33mf[39m[33m"[39m[33m Did you mean: [39m[33m'[39m[38;5;132;01m{[39;00msuggestions[[32m0[39m][38;5;132;01m}[39;00m[33m'[39m[33m?[39m[33m"[39m [38;5;28;01mif[39;00m suggestions [38;5;28;01melse[39;00m [33m"[39m[33m"[39m
E                           [32m   1338[39m         )
E                           [32m   1339[39m         logger.warning(
E                           [32m   1340[39m             [33mf[39m[33m"[39m[33mUnexpected argument [39m[33m'[39m[38;5;132;01m{[39;00marg[38;5;132;01m}[39;00m[33m'[39m[33m [39m[33m"[39m
E                           [32m   1341[39m             [33mf[39m[33m"[39m[33mprovided to ChatGoogleGenerativeAI.[39m[38;5;132;01m{[39;00msuggestion[38;5;132;01m}[39;00m[33m"[39m
E                           [32m   1342[39m         )
E                           [32m-> [39m[32m1343[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1402[39m, in [36mChatGoogleGenerativeAI.validate_environment[39m[34m(self)[39m
E                           [32m   1400[39m         google_api_key = [38;5;28mself[39m.google_api_key
E                           [32m   1401[39m transport: Optional[[38;5;28mstr[39m] = [38;5;28mself[39m.transport
E                           [32m-> [39m[32m1402[39m [38;5;28mself[39m.client = [43mgenaix[49m[43m.[49m[43mbuild_generative_service[49m[43m([49m
E                           [32m   1403[39m [43m    [49m[43mcredentials[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mcredentials[49m[43m,[49m
E                           [32m   1404[39m [43m    [49m[43mapi_key[49m[43m=[49m[43mgoogle_api_key[49m[43m,[49m
E                           [32m   1405[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m   1406[39m [43m    [49m[43mclient_options[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43mclient_options[49m[43m,[49m
E                           [32m   1407[39m [43m    [49m[43mtransport[49m[43m=[49m[43mtransport[49m[43m,[49m
E                           [32m   1408[39m [43m[49m[43m)[49m
E                           [32m   1409[39m [38;5;28mself[39m.async_client_running = [38;5;28;01mNone[39;00m
E                           [32m   1410[39m [38;5;28;01mreturn[39;00m [38;5;28mself[39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_google_genai/_genai_extension.py:276[39m, in [36mbuild_generative_service[39m[34m(credentials, api_key, client_options, client_info, transport)[39m
E                           [32m    262[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mbuild_generative_service[39m(
E                           [32m    263[39m     credentials: Optional[credentials.Credentials] = [38;5;28;01mNone[39;00m,
E                           [32m    264[39m     api_key: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m   (...)[39m[32m    267[39m     transport: Optional[[38;5;28mstr[39m] = [38;5;28;01mNone[39;00m,
E                           [32m    268[39m ) -> v1betaGenerativeServiceClient:
E                           [32m    269[39m     config = _prepare_config(
E                           [32m    270[39m         credentials=credentials,
E                           [32m    271[39m         api_key=api_key,
E                           [32m   (...)[39m[32m    274[39m         client_info=client_info,
E                           [32m    275[39m     )
E                           [32m--> [39m[32m276[39m     [38;5;28;01mreturn[39;00m [43mv1betaGenerativeServiceClient[49m[43m([49m[43m*[49m[43m*[49m[43mconfig[49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:697[39m, in [36mGenerativeServiceClient.__init__[39m[34m(self, credentials, transport, client_options, client_info)[39m
E                           [32m    688[39m     transport_init: Union[
E                           [32m    689[39m         Type[GenerativeServiceTransport],
E                           [32m    690[39m         Callable[..., GenerativeServiceTransport],
E                           [32m   (...)[39m[32m    694[39m         [38;5;28;01melse[39;00m cast(Callable[..., GenerativeServiceTransport], transport)
E                           [32m    695[39m     )
E                           [32m    696[39m     [38;5;66;03m# initialize with the provided callable or the passed in class[39;00m
E                           [32m--> [39m[32m697[39m     [38;5;28mself[39m._transport = [43mtransport_init[49m[43m([49m
E                           [32m    698[39m [43m        [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    699[39m [43m        [49m[43mcredentials_file[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    700[39m [43m        [49m[43mhost[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_api_endpoint[49m[43m,[49m
E                           [32m    701[39m [43m        [49m[43mscopes[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mscopes[49m[43m,[49m
E                           [32m    702[39m [43m        [49m[43mclient_cert_source_for_mtls[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_cert_source[49m[43m,[49m
E                           [32m    703[39m [43m        [49m[43mquota_project_id[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    704[39m [43m        [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    705[39m [43m        [49m[43malways_use_jwt_access[49m[43m=[49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
E                           [32m    706[39m [43m        [49m[43mapi_audience[49m[43m=[49m[38;5;28;43mself[39;49m[43m.[49m[43m_client_options[49m[43m.[49m[43mapi_audience[49m[43m,[49m
E                           [32m    707[39m [43m    [49m[43m)[49m
E                           [32m    709[39m [38;5;28;01mif[39;00m [33m"[39m[33masync[39m[33m"[39m [38;5;129;01mnot[39;00m [38;5;129;01min[39;00m [38;5;28mstr[39m([38;5;28mself[39m._transport):
E                           [32m    710[39m     [38;5;28;01mif[39;00m CLIENT_LOGGING_SUPPORTED [38;5;129;01mand[39;00m _LOGGER.isEnabledFor(
E                           [32m    711[39m         std_logging.DEBUG
E                           [32m    712[39m     ):  [38;5;66;03m# pragma: NO COVER[39;00m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:234[39m, in [36mGenerativeServiceGrpcTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)[39m
E                           [32m    229[39m             [38;5;28mself[39m._ssl_channel_credentials = grpc.ssl_channel_credentials(
E                           [32m    230[39m                 certificate_chain=cert, private_key=key
E                           [32m    231[39m             )
E                           [32m    233[39m [38;5;66;03m# The base transport sets the host, credentials and scopes[39;00m
E                           [32m--> [39m[32m234[39m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m
E                           [32m    235[39m [43m    [49m[43mhost[49m[43m=[49m[43mhost[49m[43m,[49m
E                           [32m    236[39m [43m    [49m[43mcredentials[49m[43m=[49m[43mcredentials[49m[43m,[49m
E                           [32m    237[39m [43m    [49m[43mcredentials_file[49m[43m=[49m[43mcredentials_file[49m[43m,[49m
E                           [32m    238[39m [43m    [49m[43mscopes[49m[43m=[49m[43mscopes[49m[43m,[49m
E                           [32m    239[39m [43m    [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m[43m,[49m
E                           [32m    240[39m [43m    [49m[43mclient_info[49m[43m=[49m[43mclient_info[49m[43m,[49m
E                           [32m    241[39m [43m    [49m[43malways_use_jwt_access[49m[43m=[49m[43malways_use_jwt_access[49m[43m,[49m
E                           [32m    242[39m [43m    [49m[43mapi_audience[49m[43m=[49m[43mapi_audience[49m[43m,[49m
E                           [32m    243[39m [43m[49m[43m)[49m
E                           [32m    245[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._grpc_channel:
E                           [32m    246[39m     [38;5;66;03m# initialize with the provided callable or the default channel[39;00m
E                           [32m    247[39m     channel_init = channel [38;5;129;01mor[39;00m [38;5;28mtype[39m([38;5;28mself[39m).create_channel
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py:100[39m, in [36mGenerativeServiceTransport.__init__[39m[34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)[39m
E                           [32m     96[39m     credentials, _ = google.auth.load_credentials_from_file(
E                           [32m     97[39m         credentials_file, **scopes_kwargs, quota_project_id=quota_project_id
E                           [32m     98[39m     )
E                           [32m     99[39m [38;5;28;01melif[39;00m credentials [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m._ignore_credentials:
E                           [32m--> [39m[32m100[39m     credentials, _ = [43mgoogle[49m[43m.[49m[43mauth[49m[43m.[49m[43mdefault[49m[43m([49m
E                           [32m    101[39m [43m        [49m[43m*[49m[43m*[49m[43mscopes_kwargs[49m[43m,[49m[43m [49m[43mquota_project_id[49m[43m=[49m[43mquota_project_id[49m
E                           [32m    102[39m [43m    [49m[43m)[49m
E                           [32m    103[39m     [38;5;66;03m# Don't apply audience if the credentials file passed from user.[39;00m
E                           [32m    104[39m     [38;5;28;01mif[39;00m [38;5;28mhasattr[39m(credentials, [33m"[39m[33mwith_gdch_audience[39m[33m"[39m):
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/google/auth/_default.py:685[39m, in [36mdefault[39m[34m(scopes, request, quota_project_id, default_scopes)[39m
E                           [32m    677[39m             _LOGGER.warning(
E                           [32m    678[39m                 [33m"[39m[33mNo project ID could be determined. Consider running [39m[33m"[39m
E                           [32m    679[39m                 [33m"[39m[33m`gcloud config set project` or setting the [39m[38;5;132;01m%s[39;00m[33m [39m[33m"[39m
E                           [32m    680[39m                 [33m"[39m[33menvironment variable[39m[33m"[39m,
E                           [32m    681[39m                 environment_vars.PROJECT,
E                           [32m    682[39m             )
E                           [32m    683[39m         [38;5;28;01mreturn[39;00m credentials, effective_project_id
E                           [32m--> [39m[32m685[39m [38;5;28;01mraise[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E                           
E                           [31mDefaultCredentialsError[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/agent.ipynb
______________ test_notebook_runs_without_errors[notebook_path4] _______________

args = (<nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bfaa689d0>, {'cell_type': 'code', 'execution_coun...urce': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}, 2)
kwargs = {'store_history': True}, name = 'MainThread'
inner = <coroutine object NotebookClient.async_execute_cell at 0x7f8bf8db3340>
loop = <_UnixSelectorEventLoop running=False closed=False debug=False>

    def wrapped(*args: Any, **kwargs: Any) -> Any:
        name = threading.current_thread().name
        inner = coro(*args, **kwargs)
        try:
>           asyncio.get_running_loop()
E           RuntimeError: no running event loop

.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:154: RuntimeError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
>           ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})

tests/test_notebooks.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:103: in preprocess
    self.preprocess_cell(cell, resources, index)
.venv/lib/python3.13/site-packages/nbconvert/preprocessors/execute.py:124: in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/jupyter_core/utils/__init__.py:158: in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/usr/lib/python3.13/asyncio/base_events.py:719: in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/nbclient/client.py:1062: in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <nbconvert.preprocessors.execute.ExecutePreprocessor object at 0x7f8bfaa689d0>
cell = {'cell_type': 'code', 'execution_count': 2, 'id': 'e0ee8f6c', 'metadata': {'execution': {'iopub.status.busy': '2025-08... 'source': 'from langchain.chat_models import init_chat_model\nllm = init_chat_model("openai:gpt-4.1", temperature=0)'}
cell_index = 2
exec_reply = {'buffers': [], 'content': {'ename': 'OpenAIError', 'engine_info': {'engine_id': -1, 'engine_uuid': '4cf974a1-4962-457...e, 'engine': '4cf974a1-4962-457f-91c0-8045ab820377', 'started': '2025-08-23T15:51:15.396466Z', 'status': 'error'}, ...}

    async def _check_raise_for_error(
        self, cell: NotebookNode, cell_index: int, exec_reply: dict[str, t.Any] | None
    ) -> None:
        if exec_reply is None:
            return None
    
        exec_reply_content = exec_reply["content"]
        if exec_reply_content["status"] != "error":
            return None
    
        cell_allows_errors = (not self.force_raise_errors) and (
            self.allow_errors
            or exec_reply_content.get("ename") in self.allow_error_names
            or "raises-exception" in cell.metadata.get("tags", [])
        )
        await run_hook(
            self.on_cell_error, cell=cell, cell_index=cell_index, execute_reply=exec_reply
        )
        if not cell_allows_errors:
>           raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
E           nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
E           ------------------
E           from langchain.chat_models import init_chat_model
E           llm = init_chat_model("openai:gpt-4.1", temperature=0)
E           ------------------
E           
E           
E           [31m---------------------------------------------------------------------------[39m
E           [31mOpenAIError[39m                               Traceback (most recent call last)
E           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E           [32m    316[39m     warnings.warn(
E           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E           [32m    320[39m         stacklevel=[32m2[39m,
E           [32m    321[39m     )
E           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E           [32m    328[39m [43m    [49m[43m)[49m
E           [32m    329[39m [38;5;28;01mif[39;00m model:
E           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E           
E               [31m[... skipping hidden 1 frame][39m
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E           [32m    787[39m         )
E           [32m    788[39m     sync_specific = {
E           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E           [32m    791[39m     }
E           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E           
E           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E           [32m    134[39m     )
E           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E           
E           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

.venv/lib/python3.13/site-packages/nbclient/client.py:918: CellExecutionError

During handling of the above exception, another exception occurred:

notebook_path = PosixPath('/workspace/notebooks/langgraph_101.ipynb')

    @pytest.mark.parametrize("notebook_path", get_notebooks())
    def test_notebook_runs_without_errors(notebook_path):
        """Test that a notebook runs without errors."""
        # Check if notebook exists
        if not notebook_path.exists():
            pytest.skip(f"Notebook {notebook_path} does not exist")
    
        print(f"Testing notebook: {notebook_path}")
    
        # Read the notebook
        with open(notebook_path, encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
    
        # Create executor
        ep = ExecutePreprocessor(timeout=600, kernel_name="python3")
    
        try:
            # Execute the notebook
            ep.preprocess(nb, {"metadata": {"path": notebook_path.parent}})
        except Exception as e:
            # Get the cell that caused the error
            for cell in nb.cells:
                if hasattr(cell, "outputs"):
                    for output in cell.outputs:
                        if output.output_type == "error":
                            error_message = "\n".join(output.traceback)
>                           pytest.fail(f"Error in notebook {notebook_path}: {error_message}")
E                           Failed: Error in notebook /workspace/notebooks/langgraph_101.ipynb: [31m---------------------------------------------------------------------------[39m
E                           [31mOpenAIError[39m                               Traceback (most recent call last)
E                           [36mCell[39m[36m [39m[32mIn[2][39m[32m, line 2[39m
E                           [32m      1[39m [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain[39;00m[34;01m.[39;00m[34;01mchat_models[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m init_chat_model
E                           [32m----> [39m[32m2[39m llm = [43minit_chat_model[49m[43m([49m[33;43m"[39;49m[33;43mopenai:gpt-4.1[39;49m[33;43m"[39;49m[43m,[49m[43m [49m[43mtemperature[49m[43m=[49m[32;43m0[39;49m[43m)[49m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:324[39m, in [36minit_chat_model[39m[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)[39m
E                           [32m    316[39m     warnings.warn(
E                           [32m    317[39m         [33mf[39m[33m"[39m[38;5;132;01m{[39;00mconfig_prefix[38;5;132;01m=}[39;00m[33m has been set but no fields are configurable. Set [39m[33m"[39m
E                           [32m    318[39m         [33mf[39m[33m"[39m[33m`configurable_fields=(...)` to specify the model params that are [39m[33m"[39m
E                           [32m    319[39m         [33mf[39m[33m"[39m[33mconfigurable.[39m[33m"[39m,
E                           [32m    320[39m         stacklevel=[32m2[39m,
E                           [32m    321[39m     )
E                           [32m    323[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m configurable_fields:
E                           [32m--> [39m[32m324[39m     [38;5;28;01mreturn[39;00m [43m_init_chat_model_helper[49m[43m([49m
E                           [32m    325[39m [43m        [49m[43mcast[49m[43m([49m[38;5;28;43mstr[39;49m[43m,[49m[43m [49m[43mmodel[49m[43m)[49m[43m,[49m
E                           [32m    326[39m [43m        [49m[43mmodel_provider[49m[43m=[49m[43mmodel_provider[49m[43m,[49m
E                           [32m    327[39m [43m        [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m,[49m
E                           [32m    328[39m [43m    [49m[43m)[49m
E                           [32m    329[39m [38;5;28;01mif[39;00m model:
E                           [32m    330[39m     kwargs[[33m"[39m[33mmodel[39m[33m"[39m] = model
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain/chat_models/base.py:351[39m, in [36m_init_chat_model_helper[39m[34m(model, model_provider, **kwargs)[39m
E                           [32m    348[39m     _check_pkg([33m"[39m[33mlangchain_openai[39m[33m"[39m)
E                           [32m    349[39m     [38;5;28;01mfrom[39;00m[38;5;250m [39m[34;01mlangchain_openai[39;00m[38;5;250m [39m[38;5;28;01mimport[39;00m ChatOpenAI
E                           [32m--> [39m[32m351[39m     [38;5;28;01mreturn[39;00m [43mChatOpenAI[49m[43m([49m[43mmodel[49m[43m=[49m[43mmodel[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           [32m    352[39m [38;5;28;01mif[39;00m model_provider == [33m"[39m[33manthropic[39m[33m"[39m:
E                           [32m    353[39m     _check_pkg([33m"[39m[33mlangchain_anthropic[39m[33m"[39m)
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py:130[39m, in [36mSerializable.__init__[39m[34m(self, *args, **kwargs)[39m
E                           [32m    128[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34m__init__[39m([38;5;28mself[39m, *args: Any, **kwargs: Any) -> [38;5;28;01mNone[39;00m:
E                           [32m    129[39m [38;5;250m    [39m[33;03m""""""[39;00m  [38;5;66;03m# noqa: D419[39;00m
E                           [32m--> [39m[32m130[39m     [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[43m.[49m[34;43m__init__[39;49m[43m([49m[43m*[49m[43margs[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43mkwargs[49m[43m)[49m
E                           
E                               [31m[... skipping hidden 1 frame][39m
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:792[39m, in [36mBaseChatOpenAI.validate_environment[39m[34m(self)[39m
E                           [32m    785[39m         [38;5;28mself[39m.http_client = httpx.Client(
E                           [32m    786[39m             proxy=[38;5;28mself[39m.openai_proxy, verify=global_ssl_context
E                           [32m    787[39m         )
E                           [32m    788[39m     sync_specific = {
E                           [32m    789[39m         [33m"[39m[33mhttp_client[39m[33m"[39m: [38;5;28mself[39m.http_client
E                           [32m    790[39m         [38;5;129;01mor[39;00m _get_default_httpx_client([38;5;28mself[39m.openai_api_base, [38;5;28mself[39m.request_timeout)
E                           [32m    791[39m     }
E                           [32m--> [39m[32m792[39m     [38;5;28mself[39m.root_client = [43mopenai[49m[43m.[49m[43mOpenAI[49m[43m([49m[43m*[49m[43m*[49m[43mclient_params[49m[43m,[49m[43m [49m[43m*[49m[43m*[49m[43msync_specific[49m[43m)[49m  [38;5;66;03m# type: ignore[arg-type][39;00m
E                           [32m    793[39m     [38;5;28mself[39m.client = [38;5;28mself[39m.root_client.chat.completions
E                           [32m    794[39m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m.async_client:
E                           
E                           [36mFile [39m[32m/workspace/.venv/lib/python3.13/site-packages/openai/_client.py:132[39m, in [36mOpenAI.__init__[39m[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)[39m
E                           [32m    130[39m     api_key = os.environ.get([33m"[39m[33mOPENAI_API_KEY[39m[33m"[39m)
E                           [32m    131[39m [38;5;28;01mif[39;00m api_key [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           [32m--> [39m[32m132[39m     [38;5;28;01mraise[39;00m OpenAIError(
E                           [32m    133[39m         [33m"[39m[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable[39m[33m"[39m
E                           [32m    134[39m     )
E                           [32m    135[39m [38;5;28mself[39m.api_key = api_key
E                           [32m    137[39m [38;5;28;01mif[39;00m organization [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
E                           
E                           [31mOpenAIError[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable

tests/test_notebooks.py:48: Failed
----------------------------- Captured stdout call -----------------------------
Testing notebook: /workspace/notebooks/langgraph_101.ipynb
----------------------------- Captured stderr call -----------------------------
ERROR:tornado.general:Uncaught exception in ZMQStream callback
Traceback (most recent call last):
  File "/workspace/.venv/lib/python3.13/site-packages/zmq/eventloop/zmqstream.py", line 565, in _log_error
    f.result()
    ~~~~~~~~^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py", line 302, in dispatch_control
    await self.process_control(msg)
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py", line 339, in process_control
    self._publish_status_and_flush("idle", "control", self.control_stream)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py", line 617, in _publish_status_and_flush
    self._publish_status(status, channel, parent)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py", line 607, in _publish_status
    self.session.send(
    ~~~~~~~~~~~~~~~~~^
        self.iopub_socket,
        ^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        ident=self._topic("status"),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/workspace/.venv/lib/python3.13/site-packages/jupyter_client/session.py", line 863, in send
    stream.send_multipart(to_send, copy=copy)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/iostream.py", line 346, in send_multipart
    return self.io_thread.send_multipart(*args, **kwargs)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/iostream.py", line 276, in send_multipart
    self.schedule(lambda: self._really_send(*args, **kwargs))
    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/iostream.py", line 269, in schedule
    f()
    ~^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/iostream.py", line 276, in <lambda>
    self.schedule(lambda: self._really_send(*args, **kwargs))
                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/ipykernel/iostream.py", line 287, in _really_send
    self.socket.send_multipart(msg, *args, **kwargs)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/zmq/sugar/socket.py", line 749, in send_multipart
    self.send(msg, zmq.SNDMORE | flags, copy=copy, track=track)
    ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.13/site-packages/zmq/sugar/socket.py", line 698, in send
    return super().send(data, flags=flags, copy=copy, track=track)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1147, in zmq.backend.cython._zmq.Socket.send
    def send(self, data, flags=0, copy: bint = True, track: bint = False):
    ^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1189, in zmq.backend.cython._zmq.Socket.send
    _check_closed(self)
    ^^^
  File "zmq/backend/cython/_zmq.py", line 1334, in zmq.backend.cython._zmq._check_closed
    raise ZMQError(ENOTSOCK)
    ^^^
zmq.error.ZMQError: Socket operation on non-socket
=============================== warnings summary ===============================
.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22
  /workspace/.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path0]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path1]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path2]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path3]
FAILED tests/test_notebooks.py::test_notebook_runs_without_errors[notebook_path4]
======================== 5 failed, 1 warning in 13.10s =========================

--- Running Python Tests (run_all_tests.py) ---

Running tests for email_assistant...
   Project: E-mail Tool Calling and Response Evaluation

Running test_response.py for email_assistant...
   Experiment: Test: test_response.py | Agent: email_assistant
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 16 items

test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] PASSED [  6%]
test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] PASSED [ 12%]
test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] PASSED [ 18%]
test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] PASSED [ 25%]
test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] PASSED [ 31%]
test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] PASSED [ 37%]
test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] PASSED [ 43%]
test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] PASSED [ 50%]
test_response.py::test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] SKIPPED [ 56%]
test_response.py::test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] SKIPPED [ 62%]
test_response.py::test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] SKIPPED [ 68%]
test_response.py::test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] SKIPPED [ 75%]
test_response.py::test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] SKIPPED [ 81%]
test_response.py::test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] SKIPPED [ 87%]
test_response.py::test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] SKIPPED [ 93%]
test_response.py::test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] SKIPPED [100%]

================== 8 passed, 8 skipped, 18 warnings in 0.86s ===================

--- Running Notebook Tests (pytest) ---
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 1 item

tests/test_notebooks.py::test_notebook_runs_without_errors[NOTSET] SKIPPED [100%]

=============================== warnings summary ===============================
.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22
  /workspace/.venv/lib/python3.13/site-packages/jupyter_client/connect.py:22: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs
  given by the platformdirs library.  To remove this warning and
  see the appropriate new directories, set the environment variable
  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.
  The use of platformdirs will be the default in `jupyter_core` v6
    from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================== 1 skipped, 1 warning in 3.43s =========================

--- Running Python Tests (run_all_tests.py) ---

Running tests for email_assistant...
   Project: E-mail Tool Calling and Response Evaluation

Running test_response.py for email_assistant...
   Experiment: Test: test_response.py | Agent: email_assistant
============================= test session starts ==============================
platform linux -- Python 3.13.3, pytest-8.4.1, pluggy-1.6.0 -- /workspace/.venv/bin/python
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
collecting ... collected 16 items

test_response.py::test_email_dataset_tool_calls[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] PASSED [  6%]
test_response.py::test_email_dataset_tool_calls[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] PASSED [ 12%]
test_response.py::test_email_dataset_tool_calls[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] PASSED [ 18%]
test_response.py::test_email_dataset_tool_calls[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] PASSED [ 25%]
test_response.py::test_email_dataset_tool_calls[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] PASSED [ 31%]
test_response.py::test_email_dataset_tool_calls[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] PASSED [ 37%]
test_response.py::test_email_dataset_tool_calls[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] PASSED [ 43%]
test_response.py::test_email_dataset_tool_calls[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] PASSED [ 50%]
test_response.py::test_response_criteria_evaluation[email_input0-email_input_1-\n\u2022 Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n-expected_calls0] SKIPPED [ 56%]
test_response.py::test_response_criteria_evaluation[email_input1-email_input_4-\n\u2022 Check calendar availability for Tuesday or Thursday afternoon next week with check_calendar_availability tool call \n\u2022 Confirm availability for a 45-minute meeting\n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email with write_email tool call to acknowledge tax planning request and notifying that a meeting has been scheduled  \n-expected_calls1] SKIPPED [ 62%]
test_response.py::test_response_criteria_evaluation[email_input2-email_input_6-\n\u2022 Express interest in attending TechConf 2025\n\u2022 Ask specific questions about AI/ML workshops\n\u2022 Inquire about group discount details\n\u2022 Send email with write_email tool call to express interest in attending TechConf 2025, ask specific questions about AI/ML workshops, and inquire about group discount details\n-expected_calls2] SKIPPED [ 68%]
test_response.py::test_response_criteria_evaluation[email_input3-email_input_7-\n\u2022 Explicitly agree to review the technical specifications\n\u2022 Acknowledge Friday deadline\n\u2022 Send email with write_email tool call to explicitly agree to review the technical specifications and acknowledge Friday deadline\n-expected_calls3] SKIPPED [ 75%]
test_response.py::test_response_criteria_evaluation[email_input4-email_input_8-\n\u2022 Send email with write_email tool call to express interest in registering daughter for swimming class\n-expected_calls4] SKIPPED [ 81%]
test_response.py::test_response_criteria_evaluation[email_input5-email_input_10-\n\u2022 Check calendar for 90-minute meeting availability for Monday or Wednesday with check_calendar_availability tool call \n\u2022 Send email acknowledging the request and providing availability with write_email tool call  \n-expected_calls5] SKIPPED [ 87%]
test_response.py::test_response_criteria_evaluation[email_input6-email_input_13-\n\u2022 Acknowledge annual checkup reminder\n\u2022 Send email with write_email tool call to acknowledge annual checkup reminder\n-expected_calls6] SKIPPED [ 93%]
test_response.py::test_response_criteria_evaluation[email_input7-email_input_15-\n\u2022 Check calendar for 60-minute meeting availability for Tuesday or Thursday with check_calendar_availability tool call \n\u2022 Send calendar invite with schedule_meeting tool call \n\u2022 Send email agreeing to collaborate on the joint presentation and notifying that a meeting has been scheduled with write_email tool call  \n-expected_calls7] SKIPPED [100%]

================== 8 passed, 8 skipped, 18 warnings in 0.88s ===================

